{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data: outbrain click prediction\n",
    "\n",
    "Tasks:\n",
    "Using Spark RDD, DataFrame API and Python, calculate:\n",
    "\n",
    "**1**. Top 10 most visited document_ids in the page_views_sample log\n",
    "\n",
    "**2**. How many users have at least 2 different traffic_sources in the page_views_sample log (note the value is not a count, it's an encoded enum)\n",
    "\n",
    "**3***. Top 10 most visited topic_ids in page_views_sample log (use documents_topics table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submission format is the result.json json file with top_10_documents, users and top_10_topics keys.\n",
    "For TOP-10 results, the answer must be written in the form of a sheet ordered from TOP-1 to TOP-10 with an id.\n",
    "\n",
    "result.json example:\n",
    "\n",
    "    {\n",
    "        \"top_10_documents\": [\n",
    "            111,\n",
    "            222,\n",
    "            333,\n",
    "            ...,\n",
    "            1010\n",
    "        ],\n",
    "        \"users\": 10000,\n",
    "        \"top_10_topics\": [\n",
    "            11,\n",
    "            22,\n",
    "            33,\n",
    "            ...,\n",
    "            101\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.2.3\n",
      "Py4J version: 0.10.9.5\n"
     ]
    }
   ],
   "source": [
    "from pyspark import __version__ as pyspark_version\n",
    "from py4j import __version__ as py4j_version\n",
    "\n",
    "print(\"PySpark version:\", pyspark_version)\n",
    "print(\"Py4J version:\", py4j_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import findspark\n",
    "import tqdm.notebook as tqdm\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc, countDistinct\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jovyan\n",
      " * Starting OpenBSD Secure Shell server sshd\n",
      "start-stop-daemon: unable to set gid to 0 (Operation not permitted)\n",
      "   ...fail!\n",
      " * sshd is running\n",
      "Starting namenodes on [localhost]\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: namenode is running as process 188.  Stop it first and ensure /tmp/hadoop-jovyan-namenode.pid file is empty before retry.\n",
      "Starting datanodes\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: datanode is running as process 293.  Stop it first and ensure /tmp/hadoop-jovyan-datanode.pid file is empty before retry.\n",
      "Starting secondary namenodes [984defca1bf6]\n",
      "984defca1bf6: Warning: Permanently added '984defca1bf6' (ED25519) to the list of known hosts.\n",
      "984defca1bf6: secondarynamenode is running as process 506.  Stop it first and ensure /tmp/hadoop-jovyan-secondarynamenode.pid file is empty before retry.\n",
      "Starting resourcemanager\n",
      "resourcemanager is running as process 713.  Stop it first and ensure /tmp/hadoop-jovyan-resourcemanager.pid file is empty before retry.\n",
      "Starting nodemanagers\n",
      "localhost: Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\n",
      "localhost: nodemanager is running as process 811.  Stop it first and ensure /tmp/hadoop-jovyan-nodemanager.pid file is empty before retry.\n",
      "WARNING: Use of this script to start the MR JobHistory daemon is deprecated.\n",
      "WARNING: Attempting to execute replacement \"mapred --daemon start\" instead.\n",
      "18897 sun.tools.jps.Jps -lm\n",
      "293 org.apache.hadoop.hdfs.server.datanode.DataNode\n",
      "713 org.apache.hadoop.yarn.server.resourcemanager.ResourceManager\n",
      "506 org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode\n",
      "811 org.apache.hadoop.yarn.server.nodemanager.NodeManager\n",
      "18859 org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer\n",
      "188 org.apache.hadoop.hdfs.server.namenode.NameNode\n",
      "Configured Capacity: 997518737408 (929.01 GB)\n",
      "Present Capacity: 718314850436 (668.98 GB)\n",
      "DFS Remaining: 717480357888 (668.21 GB)\n",
      "DFS Used: 834492548 (795.83 MB)\n",
      "DFS Used%: 0.12%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 127.0.0.1:9866 (localhost)\n",
      "Hostname: 984defca1bf6\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 997518737408 (929.01 GB)\n",
      "DFS Used: 834492548 (795.83 MB)\n",
      "Non DFS Used: 279256610684 (260.08 GB)\n",
      "DFS Remaining: 717480357888 (668.21 GB)\n",
      "DFS Used%: 0.08%\n",
      "DFS Remaining%: 71.93%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Thu Mar 23 18:48:09 UTC 2023\n",
      "Last Block Report: Thu Mar 23 16:34:48 UTC 2023\n",
      "Num of Blocks: 52\n",
      "\n",
      "\n",
      "chmod: changing permissions of '/home/jovyan/jupyter.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/nginx.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/error.log': Operation not permitted\n",
      "chmod: changing permissions of '/home/jovyan/access.log': Operation not permitted\n"
     ]
    }
   ],
   "source": [
    "# start Hadoop\n",
    "! /home/jovyan/start-hadoop.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-03-23 18:48:17,798 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark\n",
    "findspark.init()\n",
    "sc = SparkSession.builder.appName('jupyter').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                Size   Used  Available  Use%\n",
      "hdfs://localhost:9000  929.0 G  1.1 G    668.5 G    0%\n",
      "Found 2 items\n",
      "drwxrwx---   - root   supergroup          0 2023-03-23 16:54 /tmp\n",
      "drwxr-xr-x   - jovyan supergroup          0 2023-03-23 16:54 /user\n"
     ]
    }
   ],
   "source": [
    "# check HDFS status\n",
    "!hdfs dfs -df -h\n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "To use the Kaggle API, you'll need to download your kaggle.json file.\n",
    "Follow these steps:\n",
    "1. Go to the Kaggle website (https://www.kaggle.com/).\n",
    "2. Log in to your account.\n",
    "3. Click on your profile picture in the top right corner and then click on 'Account.'\n",
    "4. Scroll down to the 'API' section, and click the 'Create New API Token' button.\n",
    "5. Your kaggle.json file will be downloaded to your computer.\n",
    "6. Move the kaggle.json file to the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/jovyan/work\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current directory: {current_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files and directories:\n",
      "wiki\n",
      "images\n",
      "yandex_music\n",
      "spark-advanced-seminar-task.ipynb\n",
      "spark-ml-seminar.ipynb\n",
      "spark-ml-task.ipynb\n",
      ".ipynb_checkpoints\n",
      "kaggle.json\n",
      ".git\n",
      "lsml\n",
      "SGA_taschaste.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Check if the kaggle.json file in the current working directory\n",
    "print(\"\\nFiles and directories:\")\n",
    "for file in os.listdir(current_directory):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp /home/jovyan/work/kaggle.json ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /opt/conda/lib/python3.10/site-packages (1.5.13)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kaggle) (2.28.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kaggle) (2022.12.7)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from kaggle) (1.26.15)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kaggle) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page_views_sample.csv.zip to /home/jovyan/work\n",
      "100%|███████████████████████████████████████▊| 148M/149M [00:23<00:00, 6.73MB/s]\n",
      "100%|████████████████████████████████████████| 149M/149M [00:23<00:00, 6.53MB/s]\n",
      "Archive:  page_views_sample.csv.zip\n",
      "  inflating: page_views_sample.csv   \n",
      "put: `page_views_sample.csv': File exists\n",
      "Downloading documents_topics.csv.zip to /home/jovyan/work\n",
      " 99%|███████████████████████████████████████▋| 120M/121M [00:20<00:00, 5.80MB/s]\n",
      "100%|████████████████████████████████████████| 121M/121M [00:20<00:00, 6.16MB/s]\n",
      "Archive:  documents_topics.csv.zip\n",
      "  inflating: documents_topics.csv    \n",
      "put: `documents_topics.csv': File exists\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Get files to HDFS\n",
    "files = [\n",
    "         \"page_views_sample\", \n",
    "         \"documents_topics\"\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    !kaggle competitions download -c outbrain-click-prediction -f {file}.csv.zip\n",
    "    !unzip {file}.csv.zip\n",
    "    !hdfs dfs -put {file}.csv\n",
    "    !rm {file}.csv.zip {file}.csv\n",
    "\n",
    "print('\\nDone!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f82766fddb849108973e860a4463675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_views_sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n",
      "|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n",
      "|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n",
      "+--------------+-----------+---------+--------+------------+--------------+\n",
      "\n",
      "documents_topics\n",
      "+-----------+--------+------------------+\n",
      "|document_id|topic_id|  confidence_level|\n",
      "+-----------+--------+------------------+\n",
      "|    1595802|     140|0.0731131601068925|\n",
      "|    1595802|      16|0.0594164867373976|\n",
      "|    1595802|     143|0.0454207537554526|\n",
      "+-----------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see the data\n",
    "for name in tqdm.tqdm(files):\n",
    "    df = se.read.csv(\"{}.csv\".format(name), header=True)\n",
    "    df.registerTempTable(name)\n",
    "    print(name)\n",
    "    df.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the CSV files into PySpark DataFrames\n",
    "page_views = sc.read.csv(\"page_views_sample.csv\", header=True, inferSchema=True)\n",
    "documents_topics = sc.read.csv(\"documents_topics.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Task 1: Top 10 most visited document_ids in the page_views_sample log\n",
    "top_10_documents = (\n",
    "    page_views\n",
    "    .groupBy(\"document_id\")\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .limit(10)\n",
    "    .select(\"document_id\")\n",
    "    .rdd.map(lambda x: int(x.document_id))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Task 2: How many users have at least 2 different traffic_sources in the page_views_sample log\n",
    "users = (\n",
    "    page_views\n",
    "    .groupBy(\"uuid\")\n",
    "    .agg(countDistinct(\"traffic_source\").alias(\"distinct_traffic_sources\"))\n",
    "    .where(\"distinct_traffic_sources >= 2\")\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Task 3: Top 10 most visited topic_ids in page_views_sample log (use documents_topics table)\n",
    "top_10_topics = (\n",
    "    page_views\n",
    "    .join(documents_topics, page_views.document_id == documents_topics.document_id, 'inner')\n",
    "    .groupBy(documents_topics.topic_id)\n",
    "    .count()\n",
    "    .orderBy(desc(\"count\"))\n",
    "    .limit(10)\n",
    "    .select(\"topic_id\")\n",
    "    .rdd.map(lambda x: int(x.topic_id))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top_10_documents': [1811567, 234, 42744, 1858440, 1780813, 60164, 1790442, 1877626, 1821895, 732651], 'users': 98080, 'top_10_topics': [20, 16, 216, 136, 140, 143, 36, 97, 8, 269]}\n"
     ]
    }
   ],
   "source": [
    "# Let's see the results\n",
    "result = {\n",
    "    \"top_10_documents\": top_10_documents,\n",
    "    \"users\": users,\n",
    "    \"top_10_topics\": top_10_topics\n",
    "}\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to the JSON\n",
    "with open(\"result.json\", \"w\") as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "# Send to the test\n",
    "! curl -F file=@result.json \"51.250.54.133:80/MDS-LSML1/tascha_ste/w4/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spark_seminar (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
